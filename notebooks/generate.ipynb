{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from audiocraft.models import MultiBandDiffusion\n",
    "from audiotools import AudioSignal\n",
    "from df.enhance import enhance, init_df\n",
    "from huggingface_hub import hf_hub_download\n",
    "from vocos import Vocos\n",
    "\n",
    "from pflow_encodec.data.tokenizer import EncodecTokenizer, TextTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, device=\"cpu\"):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    model = hydra.utils.instantiate(ckpt[\"model_config\"])\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    return model, ckpt[\"data_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = hf_hub_download(repo_id=\"seastar105/pflow-encodec-libritts\", filename=\"libritts_base.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data_config = load_model(ckpt_path, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = hf_hub_download(repo_id=\"seastar105/pflow-encodec-libritts\", filename=\"prompt_samples/prompt1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = TextTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec_tokenizer = EncodecTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model, df_states, _ = init_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos_model = Vocos.from_pretrained(\"charactr/vocos-encodec-24khz\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbd_model = MultiBandDiffusion.get_mbd_24khz(bw=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def pflow_inference(\n",
    "    model, text, prompt_path, data_config, cfg_scale=1.0, nfe=16, ode_method=\"midpoint\", return_latent=False\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    prompt = encodec_tokenizer.encode_file(prompt_path).to(device)\n",
    "    mean = data_config[\"mean\"]\n",
    "    std = data_config[\"std\"]\n",
    "    upscale_ratio = data_config[\"text2latent_ratio\"]\n",
    "\n",
    "    text_token = text_tokenizer.encode_text(text).to(device).unsqueeze(0)\n",
    "    prompt = (prompt - mean) / std\n",
    "    result = model.generate(\n",
    "        text_token, prompt, cfg_scale=cfg_scale, nfe=nfe, ode_method=ode_method, upscale_ratio=upscale_ratio\n",
    "    )\n",
    "    result = result * std + mean\n",
    "    if return_latent:\n",
    "        return result.cpu()\n",
    "    recon = encodec_tokenizer.decode_latents(result.to(device=encodec_tokenizer.device, dtype=encodec_tokenizer.dtype))\n",
    "    return recon.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def mbd_decode(mbd_model, latent):\n",
    "    codes = encodec_tokenizer.quantize_latents(latent.to(device=encodec_tokenizer.device))\n",
    "    recon = mbd_model.tokens_to_wav(codes)\n",
    "    return recon.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def vocos_decode(vocos_model, latent):\n",
    "    codes = encodec_tokenizer.quantize_latents(latent.to(device=encodec_tokenizer.device)).squeeze()[:16, :]\n",
    "    features = vocos_model.codes_to_features(codes)\n",
    "    bandwidth_id = torch.tensor([3]).to(features.device)\n",
    "    audio = vocos_model.decode(features, bandwidth_id=bandwidth_id)\n",
    "    return audio.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def df_enhance(df_model, df_state, audio):\n",
    "    if audio.ndim == 3:\n",
    "        audio = audio.squeeze(0)\n",
    "    enhanced = enhance(df_model, df_state, audio)\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSignal(prompt_path).embed(display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"P-Flow encodec is Text-to-Speech model trained on Encodec latent using Flow Matching.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pflow_result = pflow_inference(model, text, prompt_path, data_config, cfg_scale=1.0, nfe=16, ode_method=\"midpoint\")\n",
    "pflow_signal = AudioSignal(pflow_result, 24000).ensure_max_of_audio()\n",
    "pflow_signal.embed(display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = encodec_tokenizer.encode_audio(pflow_result.to(encodec_tokenizer.device))\n",
    "mbd_recon = mbd_decode(mbd_model, latents)\n",
    "mbd_signal = AudioSignal(mbd_recon, 24000).ensure_max_of_audio()\n",
    "mbd_signal.embed(display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbd_df_result = df_enhance(df_model, df_states, mbd_recon)\n",
    "mbd_df_signal = AudioSignal(mbd_df_result, 24000).ensure_max_of_audio()\n",
    "mbd_df_signal.embed(display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = encodec_tokenizer.encode_audio(pflow_result.to(encodec_tokenizer.device))\n",
    "vocos_recon = vocos_decode(vocos_model, latents)\n",
    "vocos_signal = AudioSignal(vocos_recon, 24000).ensure_max_of_audio()\n",
    "vocos_signal.embed(display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos_df_result = df_enhance(df_model, df_states, vocos_recon)\n",
    "vocos_df_signal = AudioSignal(vocos_df_result, 24000).ensure_max_of_audio()\n",
    "vocos_df_signal.embed(display=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pflow-encodec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
